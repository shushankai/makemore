{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP: Multi Layer Perceptron to our Character level language model\n",
    "\n",
    "[Paper Implementation: A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "\n",
    "Use full link: \n",
    "* [PyTorch internals](https://blog.ezyang.com/2019/05/pytorch-internals/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main idea:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A goal of statistical language modeling is to learn the joint probability function of sequences of\n",
    "words in a language. -> this is intrinscially difficult because of the curse of dimensionality as our Matrix N will keep getting larger and larger \n",
    "\n",
    "The paper propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentenace to inform the model about an exponential number of semantically neighboring sentences \n",
    "\n",
    "The model learns simultaneously \n",
    "1. a distributed representation for each word along with \n",
    "2. the probability function for word sequences, expressed in terms of these representation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a Distributed Representation for Each Word?**\n",
    "\n",
    "In traditional language models (like n-grams), words are treated as discrete symbols. Each word is typically assigned a unique index, meaning the model has no inherent understanding of the relationships between words. For example:\n",
    "*\t“dog” → Index 1023\n",
    "*\t“cat” → Index 453\n",
    "*\t“apple” → Index 7890\n",
    "\n",
    "This representation is one-hot encoding, where each word is represented as a long vector with a single 1 at the corresponding index and 0s elsewhere. However, this approach has two major drawbacks:\n",
    "*\tIt is sparse (high-dimensional vectors with mostly zeros).\n",
    "*\tIt lacks semantic relationships (e.g., “dog” and “cat” are treated as completely unrelated).\n",
    "\n",
    "How Distributed Representations Solve This Problem\n",
    "\n",
    "Instead of using one-hot vectors, the neural model assigns each word a continuous, dense vector representation in a lower-dimensional space. This is known as word embeddings.\n",
    "\n",
    "For example, instead of:\n",
    "`dog → [0, 0, 0, 1, 0, 0, ..., 0]  (one-hot vector of size 100,000)`\n",
    "\n",
    "we only have \n",
    "`dog → [0.8, -1.2, 0.3, 0.5, ...]  (dense vector of size 300)`\n",
    "\n",
    "These vectors capture semantic similarities:\n",
    "*\t“dog” and “cat” will have similar vector representations because they belong to the same category (animals).\n",
    "*\t“dog” and “car” will have very different vectors because they are unrelated concepts.\n",
    "\n",
    "____________________\n",
    "Key Idea\n",
    "*\tEach word is mapped to a continuous vector space, where similar words have similar representations.\n",
    "*\tThese embeddings are learned during training and allow the model to generalize better to unseen word sequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Probability Function for Word Sequences Using These Representations**\n",
    "\n",
    "Once we have dense word embeddings, we can define a probability function for predicting the next word in a sequence.\n",
    "\n",
    "*Traditional n-gram Probability Function*\n",
    "\n",
    "In an n-gram model, the probability of a word sequence is defined as:\n",
    "\n",
    "$$P(w_t | w_{t-1}, w_{t-2}, …, w_{t-n+1})$$\n",
    "\n",
    "Where each $w_t$ is a word from the vocabulary, and probabilities are learned based on counting occurrences in a corpus.\n",
    "\n",
    "*Neural Probability Function*\n",
    "\n",
    "Instead of relying on discrete word occurrences, the neural probabilistic model defines the probability function in terms of the learned word representations:\n",
    "\n",
    "$$P(w_t | w_{t-1}, w_{t-2}, …, w_{t-n+1}) = f(\\textbf{v}{w{t-1}}, \\textbf{v}{w{t-2}}, …, \\textbf{v}{w{t-n+1}})$$\n",
    "\n",
    "where:\n",
    "*\t$\\textbf{v}_{w}$ is the word embedding of word w.\n",
    "* \tf is a neural network that learns to predict the next word given its context.\n",
    "\n",
    "Instead of looking up fixed probability tables (as in n-grams), the model learns a smooth function that can generalize to unseen sequences.\n",
    "\n",
    "Example: Predicting the Next Word\n",
    "\n",
    "Let’s say our training corpus contains:\n",
    "* \t“The cat sat on the mat.”\n",
    "* \t“The dog lay on the mat.”\n",
    "\n",
    "If the model encounters the incomplete sentence “The rabbit jumped on the ___”, traditional models might struggle because they haven’t seen “rabbit” before.\n",
    "\n",
    "However, a neural probabilistic model has distributed representations where “rabbit” has a similar vector to “cat” and “dog,” allowing it to infer that “mat” is still a likely next word.\n",
    "\n",
    "________________________\n",
    "\n",
    "*Key Takeaways*\n",
    "1.\tDistributed Representations:\n",
    "\t*\tInstead of treating words as discrete symbols, the model represents each word as a dense, continuous vector.\n",
    "\t*\tThis allows words with similar meanings to have similar representations, helping generalization.\n",
    "2.\tProbability Function in Terms of Representations:\n",
    "\t*\tInstead of counting word sequences, the model learns a function that predicts the next word based on the embeddings of previous words.\n",
    "\t*\tThis function is implemented as a neural network, which can generalize to unseen sequences.\n",
    "3.\tAdvantage Over Traditional Models:\n",
    "\t*\tThe neural model avoids the curse of dimensionality by reducing reliance on explicit probability tables.\n",
    "\t*\tIt learns semantic relationships between words, making it more robust to data sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('name.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look up for characters \n",
    "chars = sorted(set(''.join(words)))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0 \n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... -----> e\n",
      "..e -----> m\n",
      ".em -----> m\n",
      "emm -----> a\n",
      "mma -----> .\n",
      "olivia\n",
      "... -----> o\n",
      "..o -----> l\n",
      ".ol -----> i\n",
      "oli -----> v\n",
      "liv -----> i\n",
      "ivi -----> a\n",
      "via -----> .\n",
      "ava\n",
      "... -----> a\n",
      "..a -----> v\n",
      ".av -----> a\n",
      "ava -----> .\n",
      "isabella\n",
      "... -----> i\n",
      "..i -----> s\n",
      ".is -----> a\n",
      "isa -----> b\n",
      "sab -----> e\n",
      "abe -----> l\n",
      "bel -----> l\n",
      "ell -----> a\n",
      "lla -----> .\n",
      "sophia\n",
      "... -----> s\n",
      "..s -----> o\n",
      ".so -----> p\n",
      "sop -----> h\n",
      "oph -----> i\n",
      "phi -----> a\n",
      "hia -----> .\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset \n",
    "X ,Y = [], []\n",
    "\n",
    "# this is context length: how many character do we take to predict the next\n",
    "block_size = 3 \n",
    "\n",
    "for w in words[:5]:\n",
    "    \n",
    "    print(w)\n",
    "    # Setting the index for SOS \n",
    "    context = [0] * block_size\n",
    "    \n",
    "    # loop through all the chs in w with EOS as '.'\n",
    "    for chs in w + '.':\n",
    "        ix = stoi[chs]\n",
    "        X.append(context) # already known \n",
    "        Y.append(ix) # following/next character \n",
    "\n",
    "        print(f\"{''.join(itos[i] for i in context)} -----> {itos[ix]}\")\n",
    "        # updating the context \n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "# Converting to tensor\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up table for the neural net\n",
    "# in the paper they have embedded a vocab of 17000 words to as small as 17000, 300 \n",
    "# we will try to embedded it into 27, 2 \n",
    "\n",
    "# randomly intiallized the C \n",
    "C = torch.randn((27, 2)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8341, 0.2555])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets try to embedded the integer 6 (this will be index 5 as indexing in python starts at 0)\n",
    "# the embedding we have at index 5 is \n",
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8341, 0.2555])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Earlier we were using one-hot encoding to create a new vector\n",
    "# if we do the same for int 5 we will have a vector of size 27 with 1 at index 5 and rest everything as 0 \n",
    "# and when we multiply the same with C we will get the same result as indexing at C[5]\n",
    "# instead of doing one-hot encoding and doing matrix multiplication we can just index C \n",
    "\n",
    "F.one_hot(torch.tensor(5), num_classes = 27).float() @ C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0]), tensor([ 0.1190, -0.0155]), torch.Size([3, 2]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0], C[0], C[X[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the hidden layer \n",
    "W1 = torch.randn((6, 100) )\n",
    "b1 = torch.randn(100)\n",
    "\n",
    "\n",
    "# Now what we will be doing is emb @ W1 + b1 but this will through the error as \n",
    "# our embedding are of the shape 32, 3, 2 and W1 is 6, 100 \n",
    "# to solve this we can use pytoch method torch.cat -> it takes a sequence of tensor and concat them for the given dim\n",
    "\n",
    "# for all the examples give me the embedding for the 1st character in the context \n",
    "# as per the image in the paper these are the output from the first layer for neuron 0, 1, 2\n",
    "# emb[:, 0, :] , emb[:, 1, :], emb[:, 2, :] each one of them will of shape (32, 2)\n",
    "# representing the first character, second character, third character and each character represented by vector of size 2(embedding)\n",
    "# concat them at dim 1 we will get the 32, 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((emb[:, 0, :] , emb[:, 1, :], emb[:, 2, :]), 1).shape\n",
    "\n",
    "# but if we increase the block size then we will have to update the above code again \n",
    "# instead of this we can use torch.unbind -> this removes a tensor dimenssion \n",
    "# so we will remove the dim 1 and create a tuple of 3(based on the input tensor) tensor of shape 32,2  \n",
    "# and then concat them together at dim 1 to get 32, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(torch.unbind(emb, 1), 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 6]), torch.float32, torch.float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the better option view() : gives a temporary view of the shape we want make no changes in the input tensor \n",
    "emb.view(-1, 6).shape, emb.view(-1, 6).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 100]),\n",
       " torch.float32,\n",
       " tensor([[-0.0348,  0.8522, -0.3373,  0.6458, -0.1282,  0.9641,  0.1663,  0.2037,\n",
       "           0.7526, -0.8614,  0.4565,  0.7607, -0.9409, -0.8764,  0.0039, -0.9848,\n",
       "           0.9433,  0.7091, -0.2657, -0.9657, -0.2004, -0.9435, -0.3533,  0.3090,\n",
       "          -0.8595,  0.6137, -0.2199,  0.6065,  0.6908, -0.1122,  0.7290, -0.3638,\n",
       "           0.1711, -0.7295, -0.3449, -0.6122, -0.9385, -0.4828, -0.5417, -0.2458,\n",
       "          -0.3233,  0.3978,  0.6026, -0.5404, -0.7556,  0.3038, -0.6807, -0.9824,\n",
       "           0.0593,  0.8942,  0.7186, -0.9873, -0.7633, -0.5711,  0.7156,  0.1131,\n",
       "           0.8977,  0.7049,  0.3637, -0.8260,  0.9959, -0.2805,  0.9093, -0.1771,\n",
       "           0.7259, -0.9278, -0.2451,  0.9379, -0.9114,  0.9254,  0.5064, -0.1054,\n",
       "          -0.5759,  0.6350, -0.8552,  0.8523,  0.1835,  0.8706, -0.9628,  0.0686,\n",
       "          -0.7906,  0.3130,  0.9276,  0.7049, -0.9981,  0.5847,  0.6729, -0.1481,\n",
       "           0.0935, -0.1218, -0.8515, -0.4874,  0.8115,  0.6588, -0.0166,  0.3158,\n",
       "          -0.8292, -0.0551,  0.8131, -0.2937]]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.tanh((emb.view(-1, 6) @ W1 + b1))\n",
    "h.shape, h.dtype, h[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0349,  1.2642, -0.3510,  ..., -0.0552,  1.1362, -0.3026],\n",
       "        [-0.4745,  1.2872, -0.6505,  ...,  0.2471,  1.3004, -0.1889],\n",
       "        [-0.0846,  1.5484, -1.3016,  ..., -0.6944,  1.8721, -1.3949],\n",
       "        ...,\n",
       "        [ 0.0611,  2.0055,  0.8441,  ..., -2.4274,  1.8375,  0.1690],\n",
       "        [-0.1578, -0.2346,  2.2537,  ..., -0.5914, -0.5785, -0.3627],\n",
       "        [-0.4736,  1.7491, -3.0632,  ..., -2.6217,  2.2543, -0.4815]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(emb.view(-1, 6) @ W1 + b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
